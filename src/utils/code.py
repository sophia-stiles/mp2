"""Importable utilities for video-text embedding inference.

Provides four main functions:
    - get_video_embeddings(video_folder)
    - get_text_embeddings(text_folder)
    - plot_similarity_matrix(video_embeddings, text_embeddings)
    - plot_tsne(video_embeddings, text_embeddings)
    - build_tsne_baseline(...) / overlay_tsne_points(...)

Quick start::

    from src.utils.code import (
        load_model,
        get_video_embeddings,
        get_text_embeddings,
        plot_similarity_matrix,
        plot_tsne,
    )

    # 1. Load model (with optional fine-tuned checkpoint)
    load_model(checkpoint_path="params.msgpack")

    # 2. Compute embeddings — point at your folders
    video_emb, labels = get_video_embeddings("path/to/videos/")
    text_emb,  _      = get_text_embeddings("path/to/captions/")

    # 3. Visualize
    plot_similarity_matrix(video_emb, text_emb, labels=labels)
    plot_tsne(video_emb, text_emb, labels=labels)
"""

from __future__ import annotations

import json
import colorsys
import hashlib
import pickle
import random
import re
import sys
from pathlib import Path

import jax
import jax.numpy as jnp
from jax.dlpack import from_dlpack
import numpy as np

# ---------------------------------------------------------------------------
# Ensure the ``src/`` directory is importable so that internal modules
# (``data.*``, ``model.*``) resolve correctly.
# ---------------------------------------------------------------------------
_SRC_DIR = Path(__file__).resolve().parents[1]  # …/src
if str(_SRC_DIR) not in sys.path:
    sys.path.insert(0, str(_SRC_DIR))

_MODEL_CONFIG = _SRC_DIR / "config" / "gemma3_270M.yaml"

# ---------------------------------------------------------------------------
# Lazy model singleton
# ---------------------------------------------------------------------------
_model = None
_params = None


def _patch_tf_webp() -> None:
    """Patch TensorFlow for ``decode_webp`` if missing (needed by VideoPrism)."""
    try:
        import tensorflow.python.ops.image_ops_impl as _img_ops

        if not hasattr(_img_ops, "decode_webp"):
            try:
                import tensorflow_io as tfio

                _img_ops.decode_webp = tfio.image.decode_webp
            except ImportError:
                pass
    except Exception:
        pass


def load_model(checkpoint_path: str | Path | None = None) -> None:
    """Load the alignment model and optionally restore a fine-tuned checkpoint.

    This function is idempotent — calling it again after the model is already
    loaded is a no-op.  If you need to switch checkpoints, restart the Python
    process.

    Args:
        checkpoint_path: Path to a ``params.msgpack`` checkpoint file
            (e.g. downloaded from Google Drive with :func:`download_checkpoint`).
            If *None*, the pretrained model weights are used as-is.
    """
    global _model, _params

    if _model is not None:
        return  # already loaded

    _patch_tf_webp()

    from hydra.utils import instantiate
    from omegaconf import OmegaConf

    print(f"JAX devices: {jax.devices()}")

    cfg = OmegaConf.load(str(_MODEL_CONFIG))
    print(f"Loading model from config: {_MODEL_CONFIG}")

    _model = instantiate(cfg.model, _convert_="all")
    _params = _model.params

    if checkpoint_path is not None:
        from flax import serialization

        ckpt_path = Path(checkpoint_path).expanduser().resolve()
        size_mb = ckpt_path.stat().st_size / 1e6
        print(f"Loading checkpoint: {ckpt_path} ({size_mb:.1f} MB)")

        with open(ckpt_path, "rb") as f:
            raw = f.read()

        # Try standard flax deserialization first; fall back to a streaming
        # msgpack unpacker with raised size limits for large (>1 GB) files.
        try:
            loaded = serialization.from_bytes(_params, raw)
        except (ValueError, Exception) as exc:
            print(f"Standard from_bytes failed ({exc}), retrying with raised msgpack limits …")
            import io
            import msgpack
            from flax.serialization import _msgpack_ext_unpack, from_state_dict

            buf_len = len(raw) + 1024
            unpacker = msgpack.Unpacker(
                io.BytesIO(raw),
                ext_hook=_msgpack_ext_unpack,
                raw=False,
                max_buffer_size=buf_len,
                max_bin_len=buf_len,
                max_str_len=buf_len,
                max_array_len=buf_len,
                max_map_len=buf_len,
                max_ext_len=buf_len,
            )
            state_dict = next(unpacker)
            loaded = from_state_dict(_params, state_dict)
            del state_dict

        # Free the raw bytes and intermediate copies to reclaim ~2–4 GB
        del raw
        _params = jax.tree_util.tree_map(jnp.asarray, loaded)
        del loaded
        import gc as _gc
        _gc.collect()
    else:
        print("Using pretrained weights (no checkpoint)")

    # Free the pretrained adapter weights on the model object — they are
    # superseded by _params and no longer needed for inference.
    _model.adapter_params = {}

    # Store params in bfloat16 permanently to halve memory.  Inference
    # quality is unaffected; results are cast back to float32 after the
    # forward pass.
    _params = to_bfloat16(_params)

    import gc as _gc
    _gc.collect()

    print(f"Model class: {type(_model).__name__}")
    print(f"Top-level param keys: {list(_params.keys())}")


def _get_model_and_params():
    """Return the loaded (model, params), loading defaults if necessary."""
    if _model is None:
        load_model()
    return _model, _params


# ---------------------------------------------------------------------------
# Core helpers
# ---------------------------------------------------------------------------

def to_bfloat16(x):
    """Convert a JAX array or pytree of arrays to bfloat16.

    Only floating-point leaves are cast; integer and other dtypes are left
    unchanged.  Works on single arrays, nested dicts/lists (pytrees), or any
    structure ``jax.tree_util`` can traverse.
    """
    def _cast(leaf):
        if hasattr(leaf, "dtype") and jnp.issubdtype(leaf.dtype, jnp.floating):
            return leaf.astype(jnp.bfloat16)
        return leaf
    return jax.tree_util.tree_map(_cast, x)


def l2_normalize(x: jax.Array, eps: float = 1e-8) -> jax.Array:
    """L2-normalize along the last axis."""
    return x / (jnp.linalg.norm(x, axis=-1, keepdims=True) + eps)


def _parse_timecode_to_seconds(value: str | float | int) -> float | None:
    """Parse a timestamp into seconds.

    Supports:
      - ``HH:MM:SS(.mmm)``, e.g. ``0:05:46.400``
      - ``MM:SS(.mmm)``
      - numeric values already in seconds
    """
    if isinstance(value, (int, float)):
        return float(value)
    if not isinstance(value, str):
        return None

    text = value.strip()
    if not text:
        return None

    # Fast path: already numeric text in seconds.
    try:
        return float(text)
    except ValueError:
        pass

    parts = text.split(":")
    try:
        if len(parts) == 3:
            hours = int(parts[0])
            minutes = int(parts[1])
            seconds = float(parts[2])
            return hours * 3600.0 + minutes * 60.0 + seconds
        if len(parts) == 2:
            minutes = int(parts[0])
            seconds = float(parts[1])
            return minutes * 60.0 + seconds
    except ValueError:
        return None
    return None


def _extract_annotation_intervals(
    data: dict,
    segment_key: str = "video_descriptions",
) -> list[tuple[float, float]]:
    """Extract sorted ``(start_s, end_s)`` intervals from annotation JSON."""
    raw_segments = data.get(segment_key)
    if not isinstance(raw_segments, list):
        return []

    intervals: list[tuple[float, float]] = []
    for seg in raw_segments:
        if not isinstance(seg, dict):
            continue
        start_s = _parse_timecode_to_seconds(seg.get("t0"))
        end_s = _parse_timecode_to_seconds(seg.get("t1"))
        if start_s is None or end_s is None:
            continue
        if end_s <= start_s:
            continue
        intervals.append((float(start_s), float(end_s)))

    intervals.sort(key=lambda x: (x[0], x[1]))
    return intervals


def _extract_annotation_segment_texts(
    data: dict,
    segment_key: str = "video_descriptions",
    segment_text_key: str = "text",
) -> list[tuple[float, float, str]]:
    """Extract sorted ``(start_s, end_s, text)`` items from annotation JSON."""
    raw_segments = data.get(segment_key)
    if not isinstance(raw_segments, list):
        return []

    items: list[tuple[float, float, str]] = []
    for seg in raw_segments:
        if not isinstance(seg, dict):
            continue
        start_s = _parse_timecode_to_seconds(seg.get("t0"))
        end_s = _parse_timecode_to_seconds(seg.get("t1"))
        text = seg.get(segment_text_key)
        if start_s is None or end_s is None:
            continue
        if end_s <= start_s:
            continue
        if not isinstance(text, str):
            continue
        text = text.strip()
        if not text:
            continue
        items.append((float(start_s), float(end_s), text))

    items.sort(key=lambda x: (x[0], x[1]))
    return items


# ---------------------------------------------------------------------------
# Embedding functions
# ---------------------------------------------------------------------------

def get_video_embeddings(
    video_folder: str | Path,
    *,
    annotation_folder: str | Path | None = None,
    segment_key: str = "video_descriptions",
    min_interval_segments: int = 2,
    segment_progress_every: int = 100,
    model=None,
    params=None,
    num_frames: int = 16,
    resolution: tuple[int, int] = (288, 288),
    seed: int = 42,
    batch_size: int = 1,
) -> tuple[jax.Array, list[str]]:
    """Compute L2-normalized video embeddings for every ``.mp4`` in a folder.

    Files are discovered as ``video_folder/*.mp4`` and sorted by filename stem
    so that the ordering is consistent with :func:`get_text_embeddings` when
    both folders contain files with the same names.

    If a matching annotation file ``<stem>.json`` exists, the function extracts
    time segments from ``segment_key`` and decodes interval clips by calling
    :func:`data.utils.decode` with ``frame_sampling_method="interval"``.
    Overlapping segments are supported and processed independently.

    If no annotation exists, segment extraction fails, or the number of valid
    segments is below ``min_interval_segments``, the function falls back to
    full-video decoding (single embedding for that video).

    Args:
        video_folder: Path to a directory containing ``.mp4`` video files.
        annotation_folder: Directory with ``.json`` annotations. If *None*,
            defaults to ``video_folder``.
        segment_key: JSON key that contains a list of segments with ``t0``/``t1``.
        min_interval_segments: Minimum number of valid intervals required to use
            interval decoding; otherwise full-video fallback is used.
        segment_progress_every: Print interval progress every N decoded segment
            attempts (in addition to first/last segment per video).
        model: Pre-loaded model object.  If *None*, uses the global singleton.
        params: Pre-loaded model params.  Must be provided together with *model*.
        num_frames: Number of frames to sample per video.
        resolution: Spatial resolution ``(H, W)`` for decoded frames.
        seed: Random seed for frame sampling.
        batch_size: Videos per forward-pass batch (default 1 to save RAM).

    Returns:
        ``(embeddings, labels)`` — only successfully decoded videos are included.
    """
    import gc

    import torch

    from data.utils import decode

    if model is None or params is None:
        model, params = _get_model_and_params()

    video_folder = Path(video_folder)
    annotation_folder = Path(annotation_folder) if annotation_folder is not None else video_folder
    video_files = sorted(video_folder.glob("*.mp4"), key=lambda p: p.stem)
    if not video_files:
        raise FileNotFoundError(f"No .mp4 files found in {video_folder}")

    print(f"Found {len(video_files)} videos in {video_folder}")
    print(f"Using annotations from: {annotation_folder}")

    rng = random.Random(seed)

    @jax.jit
    def _jit_video_forward(params, video_input):
        return model.get_adapted_video_embeddings(params, video_input, train=False)

    def _flush_batch(
        pending_tensors: list[torch.Tensor],
        pending_labels: list[str],
        emb_list: list[jax.Array],
        labels: list[str],
    ) -> None:
        """Run one forward pass on pending tensors and append outputs."""
        if not pending_tensors:
            return
        jax_frames = [from_dlpack(t.contiguous()) for t in pending_tensors]
        del pending_tensors[:]
        gc.collect()

        video_input = jnp.stack(jax_frames, axis=0)
        del jax_frames
        gc.collect()

        v_emb = _jit_video_forward(params, video_input)
        emb_list.append(np.asarray(l2_normalize(v_emb.astype(jnp.float32))))
        labels.extend(pending_labels)
        del pending_labels[:]

        del video_input, v_emb
        gc.collect()

    # Build a quick plan first so progress can include total segment count.
    decode_plan: list[tuple[Path, list[tuple[float, float]], bool]] = []
    total_interval_segments = 0
    for vf in video_files:
        ann_path = annotation_folder / f"{vf.stem}.json"
        intervals: list[tuple[float, float]] = []
        if ann_path.exists():
            try:
                with open(ann_path) as f:
                    ann_data = json.load(f)
                intervals = _extract_annotation_intervals(ann_data, segment_key=segment_key)
            except Exception:
                intervals = []
        use_intervals = len(intervals) >= min_interval_segments
        if use_intervals:
            total_interval_segments += len(intervals)
        decode_plan.append((vf, intervals, use_intervals))

    print(
        f"Planned interval decoding: {total_interval_segments} segments "
        f"(fallback videos: {sum(1 for _vf, _itv, use_itv in decode_plan if not use_itv)})"
    )
    if segment_progress_every < 1:
        segment_progress_every = 1

    emb_list: list[jax.Array] = []
    labels: list[str] = []
    pending_tensors: list[torch.Tensor] = []
    pending_labels: list[str] = []
    global_segment_done = 0
    global_segment_ok = 0

    for idx, (vf, intervals, use_intervals) in enumerate(decode_plan, start=1):
        print(f"  Processing video {idx} / {len(video_files)}: {vf.name} …")
        ann_path = annotation_folder / f"{vf.stem}.json"
        if not ann_path.exists():
            print(f"    NOTE: no annotation for {vf.name} at {ann_path.name} — full-video fallback")
        elif not use_intervals and len(intervals) == 0:
            # Distinguish parse failure / missing key from explicit short list.
            try:
                with open(ann_path) as f:
                    ann_data = json.load(f)
                if segment_key not in ann_data:
                    print(f"    NOTE: key '{segment_key}' not found in {ann_path.name} — full-video fallback")
            except Exception as exc:
                print(f"    WARNING: failed to parse {ann_path.name}: {exc} — full-video fallback")

        if use_intervals:
            print(f"    Using {len(intervals)} interval segments from '{segment_key}'")
            for seg_idx, (start_s, end_s) in enumerate(intervals):
                global_segment_done += 1
                if (
                    seg_idx == 0
                    or seg_idx + 1 == len(intervals)
                    or global_segment_done % segment_progress_every == 0
                ):
                    print(
                        "    Segment progress: "
                        f"video {seg_idx + 1}/{len(intervals)} | "
                        f"global {global_segment_done}/{total_interval_segments} "
                        f"(ok={global_segment_ok})"
                    )
                try:
                    video_tensor, _meta = decode(
                        str(vf),
                        num_frames,
                        resolution,
                        decode_method="pyav",
                        resize_method="center_crop_resize",
                        frame_sampling_method="interval",
                        output_range="unit",
                        dtype=torch.bfloat16,
                        rng=rng,
                        interval=(start_s, end_s),
                    )
                    if video_tensor.shape[0] == 0:
                        print(
                            f"    WARNING: {vf.name} segment {seg_idx} [{start_s:.3f}, {end_s:.3f}] "
                            "decoded to 0 frames — skipping"
                        )
                        continue
                    pending_tensors.append(video_tensor)
                    pending_labels.append(f"{vf.stem}#seg{seg_idx:03d}")
                    global_segment_ok += 1
                except Exception as exc:
                    print(f"    WARNING: failed interval decode for {vf.name} seg {seg_idx}: {exc} — skipping")
                    continue

                if len(pending_tensors) >= batch_size:
                    _flush_batch(pending_tensors, pending_labels, emb_list, labels)
        else:
            if ann_path.exists():
                print(
                    f"    NOTE: only {len(intervals)} valid segments (< {min_interval_segments}); "
                    "using full-video fallback"
                )
            try:
                video_tensor, _meta = decode(
                    str(vf),
                    num_frames,
                    resolution,
                    decode_method="pyav",
                    resize_method="center_crop_resize",
                    frame_sampling_method="max_stride",
                    output_range="unit",
                    dtype=torch.bfloat16,
                    rng=rng,
                )
                if video_tensor.shape[0] == 0:
                    print(f"    WARNING: {vf.name} decoded to 0 frames — skipping")
                    continue
                pending_tensors.append(video_tensor)
                pending_labels.append(vf.stem)
            except Exception as exc:
                print(f"    WARNING: failed to decode {vf.name}: {exc} — skipping")
                continue

            if len(pending_tensors) >= batch_size:
                _flush_batch(pending_tensors, pending_labels, emb_list, labels)

    _flush_batch(pending_tensors, pending_labels, emb_list, labels)

    if not emb_list:
        raise RuntimeError(
            f"All videos in {video_folder} failed to decode. "
            "Check that they are valid .mp4 files with enough frames."
        )

    print(f"  Successfully embedded {len(labels)} / {len(video_files)} videos")
    if total_interval_segments > 0:
        print(
            "  Interval decode totals: "
            f"attempted={global_segment_done}, successful={global_segment_ok}, "
            f"failed={global_segment_done - global_segment_ok}"
        )
    return jnp.asarray(np.concatenate(emb_list, axis=0)), labels


def _extract_caption(data: dict, caption_key: str) -> str | None:
    """Extract a plain-text caption from a JSON annotation dict.

    Handles two formats:
      1. Simple:  ``{"summary": "A dog catches a ball."}``
      2. Nested:  ``{"summary": "[{\\"summary_multimodal\\": \\"...\\" }]"}``
         where the value is a **stringified** JSON array.  In that case the
         function parses the inner JSON and looks for ``summary_multimodal``,
         then ``summary_video_only``, then ``summary``, then falls back to
         the first string value it finds.

    Returns *None* if no caption could be extracted.
    """
    raw = data.get(caption_key)
    if raw is None:
        return None

    # If the value is already a clean string (not JSON-encoded), use it.
    if isinstance(raw, str):
        stripped = raw.strip()
        # Detect stringified JSON (starts with '[' or '{')
        if stripped and stripped[0] in ("[", "{"):
            try:
                inner = json.loads(stripped)
                # Unwrap single-element list
                if isinstance(inner, list) and len(inner) > 0:
                    inner = inner[0]
                if isinstance(inner, dict):
                    # Prefer multimodal > video_only > summary > first string
                    for key in ("summary_multimodal", "summary_video_only", "summary"):
                        if key in inner and isinstance(inner[key], str):
                            return inner[key]
                    # Fallback: grab the first string value
                    for v in inner.values():
                        if isinstance(v, str) and len(v) > 20:
                            return v
                # If inner is itself a string, use it
                if isinstance(inner, str):
                    return inner
            except (json.JSONDecodeError, TypeError):
                pass  # not valid JSON — treat as plain text
        return raw

    return None


def get_text_embeddings(
    text_folder: str | Path,
    *,
    model=None,
    params=None,
    caption_key: str = "summary",
    segment_level: bool = False,
    segment_key: str = "video_descriptions",
    segment_text_key: str = "text",
    min_interval_segments: int = 2,
    fallback_caption_key: str = "summary_multimodal",
    batch_size: int = 32,
) -> tuple[jax.Array, list[str]]:
    """Compute L2-normalized text embeddings from ``.json`` caption files.

    Each JSON file should have a top-level key (default ``"summary"``) whose
    value is either a plain caption string **or** a stringified JSON array
    containing nested summary fields (``summary_multimodal``, etc.).  The
    function auto-detects the format and extracts the best available caption.

    Args:
        text_folder: Path to a directory containing ``.json`` caption files.
        model: Pre-loaded model object.  If *None*, uses the global singleton.
        params: Pre-loaded model params.  Must be provided together with *model*.
        caption_key: The JSON key that holds the caption text.
        segment_level: If *True*, extract text per segment from ``segment_key``
            and emit labels like ``<stem>#seg000``.
        segment_key: JSON key containing segment objects with ``t0``/``t1`` and
            segment text.
        segment_text_key: Key inside each segment object to embed as text.
        min_interval_segments: Minimum number of valid segments required to use
            segment-level text; otherwise fallback to single caption text.
        fallback_caption_key: Caption key to use for fallback when segment-level
            extraction is unavailable or too short.
        batch_size: Texts per forward-pass batch.

    Returns:
        ``(embeddings, labels)`` where *embeddings* is a JAX array of shape
        ``(N, D)`` and *labels* is a list of filename stems (without extension).
    """
    if model is None or params is None:
        model, params = _get_model_and_params()

    text_folder = Path(text_folder)
    json_files = sorted(text_folder.glob("*.json"), key=lambda p: p.stem)
    if not json_files:
        raise FileNotFoundError(f"No .json files found in {text_folder}")

    labels: list[str] = []
    texts: list[str] = []
    for jf in json_files:
        with open(jf) as f:
            data = json.load(f)
        if segment_level:
            segment_items = _extract_annotation_segment_texts(
                data,
                segment_key=segment_key,
                segment_text_key=segment_text_key,
            )
            if len(segment_items) >= min_interval_segments:
                print(
                    f"  {jf.stem}: using {len(segment_items)} segment texts "
                    f"from '{segment_key}.{segment_text_key}'"
                )
                for seg_idx, (_start_s, _end_s, seg_text) in enumerate(segment_items):
                    labels.append(f"{jf.stem}#seg{seg_idx:03d}")
                    texts.append(seg_text)
                continue

            print(
                f"  {jf.stem}: only {len(segment_items)} valid segments "
                f"(< {min_interval_segments}) — using fallback caption"
            )

        caption = _extract_caption(data, fallback_caption_key)
        if caption is None:
            caption = _extract_caption(data, caption_key)
        if caption is None:
            print(f"  Warning: {jf.name} — could not extract caption, skipping")
            continue
        labels.append(jf.stem)
        texts.append(caption)
        preview = caption[:80].replace("\n", " ")
        print(f"  {jf.stem}: \"{preview}…\"")

    if not texts:
        raise ValueError(
            f"No captions found — none of the JSON files in {text_folder} "
            f"contain a usable caption under key '{caption_key}'"
        )

    print(f"Found {len(texts)} captions in {text_folder}")

    import gc

    emb_list: list[jax.Array] = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i : i + batch_size]
        print(f"  Processing texts {i + 1}–{i + len(batch_texts)} / {len(texts)} …")
        tokenized = model.tokenize(batch_texts)
        tokenized = to_bfloat16(tokenized)
        t_emb = model.get_adapted_text_embeddings(params, tokenized, train=False)
        # Move to numpy immediately so the JAX device buffer is freed.
        emb_list.append(np.asarray(l2_normalize(t_emb.astype(jnp.float32))))
        del tokenized, t_emb
        gc.collect()

    return jnp.asarray(np.concatenate(emb_list, axis=0)), labels


# ---------------------------------------------------------------------------
# Visualization
# ---------------------------------------------------------------------------

def plot_similarity_matrix(
    video_embeddings: jax.Array | np.ndarray,
    text_embeddings: jax.Array | np.ndarray,
    *,
    labels: list[str] | None = None,
    show: bool = True,
):
    """Plot the cosine-similarity matrix between text and video embeddings.

    Args:
        video_embeddings: Array of shape ``(N, D)``.
        text_embeddings: Array of shape ``(M, D)`` (typically ``M == N``).
        labels: Optional tick labels for each sample.

    Returns:
        The matplotlib ``Figure`` when ``show=False``; otherwise ``None``.
    """
    import matplotlib.pyplot as plt

    video_np = np.asarray(video_embeddings)
    text_np = np.asarray(text_embeddings)
    sim_matrix = text_np @ video_np.T
    n_text, n_video = sim_matrix.shape

    if labels is None:
        labels = [f"sample_{i}" for i in range(max(n_text, n_video))]

    fig, ax = plt.subplots(
        figsize=(max(5, n_video * 0.8), max(4, n_text * 0.7))
    )
    im = ax.imshow(sim_matrix, cmap="viridis", aspect="auto")
    ax.set_xticks(range(n_video))
    ax.set_yticks(range(n_text))
    ax.set_xticklabels(labels[:n_video], rotation=45, ha="right", fontsize=8)
    ax.set_yticklabels(labels[:n_text], fontsize=8)
    ax.set_xlabel("Video")
    ax.set_ylabel("Text")
    ax.set_title("Text\u2013Video Cosine Similarity")

    # Annotate cells
    for i in range(n_text):
        for j in range(n_video):
            color = "white" if sim_matrix[i, j] < sim_matrix.mean() else "black"
            ax.text(
                j, i, f"{sim_matrix[i, j]:.2f}",
                ha="center", va="center", fontsize=7, color=color,
            )

    plt.colorbar(im, label="cosine similarity")
    plt.tight_layout()
    if show:
        plt.show()

    # Retrieval statistics
    n = min(n_text, n_video)
    diag_mean = float(np.mean(np.diag(sim_matrix[:n, :n])))
    mask = ~np.eye(n, dtype=bool)
    off_diag_mean = float(np.mean(sim_matrix[:n, :n][mask])) if n > 1 else float("nan")
    print(f"Diagonal mean:     {diag_mean:.4f}")
    print(f"Off-diagonal mean: {off_diag_mean:.4f}")
    print(f"Gap:               {diag_mean - off_diag_mean:.4f}")

    return None if show else fig


def align_embeddings_by_labels(
    video_embeddings: jax.Array | np.ndarray,
    video_labels: list[str],
    text_embeddings: jax.Array | np.ndarray,
    text_labels: list[str],
) -> tuple[np.ndarray, np.ndarray, list[str]]:
    """Align video/text embeddings by shared labels in video-label order.

    This is useful before plotting when a few items were skipped on one side
    (for example, failed segment decode on video side).
    """
    video_np = np.asarray(video_embeddings)
    text_np = np.asarray(text_embeddings)
    if video_np.shape[0] != len(video_labels):
        raise ValueError("video_embeddings row count must match len(video_labels)")
    if text_np.shape[0] != len(text_labels):
        raise ValueError("text_embeddings row count must match len(text_labels)")

    text_index = {lab: i for i, lab in enumerate(text_labels)}
    keep_video_indices: list[int] = []
    keep_text_indices: list[int] = []
    keep_labels: list[str] = []

    for i, lab in enumerate(video_labels):
        j = text_index.get(lab)
        if j is None:
            continue
        keep_video_indices.append(i)
        keep_text_indices.append(j)
        keep_labels.append(lab)

    if not keep_labels:
        raise ValueError("No shared labels found between video_labels and text_labels")

    dropped_v = len(video_labels) - len(keep_video_indices)
    dropped_t = len(text_labels) - len(keep_text_indices)
    print(
        f"Aligned by labels: kept={len(keep_labels)}, "
        f"dropped_video={dropped_v}, dropped_text={dropped_t}"
    )
    return (
        video_np[np.asarray(keep_video_indices)],
        text_np[np.asarray(keep_text_indices)],
        keep_labels,
    )


def plot_tsne(
    video_embeddings: jax.Array | np.ndarray,
    text_embeddings: jax.Array | np.ndarray,
    *,
    labels: list[str] | None = None,
    seed: int = 42,
    perplexity: float | None = None,
    axis_padding: float = 0.10,
    figsize: tuple[float, float] = (10, 8),
    show: bool = True,
):
    """Plot a joint t-SNE of video and text embeddings.

    Video embeddings are shown as circles, text as crosses.
    Matched pairs (same index) are connected by dashed lines.

    Args:
        video_embeddings: Array of shape ``(N, D)``.
        text_embeddings: Array of shape ``(N, D)``.
        labels: Optional label per sample (length ``N``).
        seed: Random seed for t-SNE.
        perplexity: t-SNE perplexity; auto-chosen if *None*.
        axis_padding: Extra margin added around t-SNE points as a fraction of
            x/y data range (e.g., 0.10 adds 10% padding on each side).
        figsize: Figure size passed to ``plt.subplots``.

    Returns:
        The matplotlib ``Figure`` when ``show=False``; otherwise ``None``.
    """
    import matplotlib.pyplot as plt
    from sklearn.manifold import TSNE

    video_np = np.asarray(video_embeddings)
    text_np = np.asarray(text_embeddings)
    n = video_np.shape[0]

    if labels is None:
        labels = [f"sample_{i}" for i in range(n)]
    assert len(labels) == n, f"Expected {n} labels, got {len(labels)}"

    all_emb = np.concatenate([video_np, text_np], axis=0)  # (2N, D)
    modality = ["video"] * n + ["text"] * n
    all_labels = list(labels) + list(labels)

    if perplexity is None:
        perplexity = min(5, max(2, len(all_emb) - 1))

    tsne = TSNE(
        n_components=2,
        perplexity=perplexity,
        random_state=seed,
        init="pca",
        learning_rate="auto",
    )
    coords = tsne.fit_transform(all_emb)

    fig, ax = plt.subplots(figsize=figsize)
    unique_labels = sorted(set(labels))
    cmap = plt.cm.get_cmap("tab10", len(unique_labels))
    label_to_color = {lab: cmap(i) for i, lab in enumerate(unique_labels)}

    for i in range(len(all_emb)):
        color = label_to_color[all_labels[i]]
        marker = "o" if modality[i] == "video" else "x"
        ax.scatter(
            coords[i, 0], coords[i, 1],
            c=[color], marker=marker, s=80,
            edgecolors="k" if marker == "o" else "none", linewidths=0.5,
        )

    # Lines connecting matched video-text pairs
    for i in range(n):
        ax.plot(
            [coords[i, 0], coords[n + i, 0]],
            [coords[i, 1], coords[n + i, 1]],
            color=label_to_color[labels[i]], alpha=0.3, linestyle="--", linewidth=0.8,
        )

    # Expand axis limits so dense clusters and labels are easier to inspect.
    x_min, x_max = float(np.min(coords[:, 0])), float(np.max(coords[:, 0]))
    y_min, y_max = float(np.min(coords[:, 1])), float(np.max(coords[:, 1]))
    x_range = max(x_max - x_min, 1e-8)
    y_range = max(y_max - y_min, 1e-8)
    x_pad = x_range * max(axis_padding, 0.0)
    y_pad = y_range * max(axis_padding, 0.0)
    ax.set_xlim(x_min - x_pad, x_max + x_pad)
    ax.set_ylim(y_min - y_pad, y_max + y_pad)

    # Legend for labels
    for lab in unique_labels:
        ax.scatter([], [], c=[label_to_color[lab]], marker="s", label=lab)
    # Legend for modality
    ax.scatter([], [], c="gray", marker="o", label="video", edgecolors="k", linewidths=0.5)
    ax.scatter([], [], c="gray", marker="x", label="text")

    ax.legend(bbox_to_anchor=(1.02, 1), loc="upper left", fontsize=8)
    ax.set_title("t-SNE: Video (o) & Text (x) Embeddings")
    ax.set_xlabel("t-SNE 1")
    ax.set_ylabel("t-SNE 2")
    plt.tight_layout()
    if show:
        plt.show()

    return None if show else fig


def _to_numpy_2d(x: jax.Array | np.ndarray, *, name: str) -> np.ndarray:
    """Convert input to a 2D numpy array."""
    arr = np.asarray(x)
    if arr.ndim != 2:
        raise ValueError(f"{name} must be 2D with shape (N, D); got shape {arr.shape}")
    return arr


def _strip_segment_suffix(label: str) -> str:
    """Map labels like '<video>#seg003' to '<video>'."""
    return re.sub(r"#seg\d+$", "", str(label))


def _stable_color_for_video_id(video_id: str) -> tuple[float, float, float]:
    """Generate a deterministic RGB color for a video id."""
    digest = hashlib.md5(video_id.encode("utf-8")).hexdigest()
    hue = int(digest[:8], 16) / float(16**8 - 1)
    # Keep saturation/value fixed for readability and contrast.
    return colorsys.hsv_to_rgb(hue, 0.65, 0.90)


def save_embeddings_npz(
    output_path: str | Path,
    video_embeddings: jax.Array | np.ndarray,
    video_labels: list[str],
    *,
    text_embeddings: jax.Array | np.ndarray | None = None,
    text_labels: list[str] | None = None,
) -> Path:
    """Save embeddings + labels into a compressed .npz file.

    Saved keys:
      - video_embeddings (N_video, D)
      - video_labels (N_video,)
      - text_embeddings (N_text, D), optional
      - text_labels (N_text,), optional
    """
    output = Path(output_path).expanduser().resolve()
    output.parent.mkdir(parents=True, exist_ok=True)

    video_np = _to_numpy_2d(video_embeddings, name="video_embeddings")
    if len(video_labels) != video_np.shape[0]:
        raise ValueError("len(video_labels) must match video_embeddings rows")

    payload: dict[str, np.ndarray] = {
        "video_embeddings": video_np.astype(np.float32, copy=False),
        "video_labels": np.asarray(video_labels, dtype=str),
    }

    if text_embeddings is not None:
        text_np = _to_numpy_2d(text_embeddings, name="text_embeddings")
        payload["text_embeddings"] = text_np.astype(np.float32, copy=False)
        if text_labels is None:
            text_labels = [f"text_{i}" for i in range(text_np.shape[0])]
        if len(text_labels) != text_np.shape[0]:
            raise ValueError("len(text_labels) must match text_embeddings rows")
        payload["text_labels"] = np.asarray(text_labels, dtype=str)
    elif text_labels is not None:
        raise ValueError("text_labels was provided but text_embeddings is None")

    np.savez_compressed(output, **payload)
    return output


def load_embeddings_npz(npz_path: str | Path) -> dict[str, np.ndarray | list[str] | None]:
    """Load embedding arrays from .npz with tolerant key aliases."""
    path = Path(npz_path).expanduser().resolve()
    with np.load(path, allow_pickle=True) as data:
        keys = set(data.files)

        def _pick(candidates: list[str], required: bool) -> np.ndarray | None:
            for key in candidates:
                if key in keys:
                    return np.asarray(data[key])
            if required:
                raise KeyError(f"Missing required key. Tried {candidates}, found {sorted(keys)}")
            return None

        video_embeddings = _pick(["video_embeddings", "video_embeds", "video_emb"], required=True)
        video_labels = _pick(["video_labels", "labels", "video_ids"], required=True)
        text_embeddings = _pick(["text_embeddings", "text_embeds", "text_emb"], required=False)
        text_labels = _pick(["text_labels", "caption_labels"], required=False)

    video_np = _to_numpy_2d(video_embeddings, name="video_embeddings")
    video_labels_list = [str(x) for x in np.asarray(video_labels).tolist()]
    if len(video_labels_list) != video_np.shape[0]:
        raise ValueError("video_labels length does not match video_embeddings rows")

    text_np: np.ndarray | None = None
    text_labels_list: list[str] | None = None
    if text_embeddings is not None:
        text_np = _to_numpy_2d(text_embeddings, name="text_embeddings")
        if text_labels is None:
            text_labels_list = [f"text_{i}" for i in range(text_np.shape[0])]
        else:
            text_labels_list = [str(x) for x in np.asarray(text_labels).tolist()]
            if len(text_labels_list) != text_np.shape[0]:
                raise ValueError("text_labels length does not match text_embeddings rows")

    return {
        "video_embeddings": video_np.astype(np.float32, copy=False),
        "video_labels": video_labels_list,
        "text_embeddings": None if text_np is None else text_np.astype(np.float32, copy=False),
        "text_labels": text_labels_list,
    }


def build_tsne_baseline(
    video_embeddings: jax.Array | np.ndarray,
    video_labels: list[str],
    *,
    text_embeddings: jax.Array | np.ndarray | None = None,
    text_labels: list[str] | None = None,
    perplexity: float = 30.0,
    metric: str = "cosine",
    random_state: int = 42,
    standardize: bool = True,
) -> dict:
    """Fit an openTSNE baseline that can later transform new points.

    Returns a pickle-serializable session dict containing:
      - baseline_coords: (N, 2) baseline t-SNE coordinates
      - labels/modality/video_ids for baseline points
      - tsne_model: fitted openTSNE embedding object (supports transform)
      - scaler: fitted StandardScaler or None
    """
    try:
        from openTSNE import TSNE
    except ImportError as exc:
        raise ImportError(
            "openTSNE is required for incremental t-SNE transforms. "
            "Install with: pip install openTSNE"
        ) from exc

    video_np = _to_numpy_2d(video_embeddings, name="video_embeddings")
    if len(video_labels) != video_np.shape[0]:
        raise ValueError("len(video_labels) must match video_embeddings rows")

    labels: list[str] = [str(x) for x in video_labels]
    modalities: list[str] = ["video"] * len(labels)
    all_parts: list[np.ndarray] = [video_np]

    if text_embeddings is not None:
        text_np = _to_numpy_2d(text_embeddings, name="text_embeddings")
        if text_labels is None:
            text_labels = [f"text_{i}" for i in range(text_np.shape[0])]
        if len(text_labels) != text_np.shape[0]:
            raise ValueError("len(text_labels) must match text_embeddings rows")
        all_parts.append(text_np)
        labels.extend([str(x) for x in text_labels])
        modalities.extend(["text"] * text_np.shape[0])

    all_emb = np.concatenate(all_parts, axis=0).astype(np.float32, copy=False)

    scaler = None
    all_emb_proc = all_emb
    if standardize:
        from sklearn.preprocessing import StandardScaler

        scaler = StandardScaler()
        all_emb_proc = scaler.fit_transform(all_emb).astype(np.float32, copy=False)

    tsne = TSNE(
        n_components=2,
        perplexity=perplexity,
        metric=metric,
        random_state=random_state,
        initialization="pca",
    )
    embedding = tsne.fit(all_emb_proc)
    coords = np.asarray(embedding, dtype=np.float32)
    video_ids = [_strip_segment_suffix(lab) for lab in labels]

    return {
        "baseline_coords": coords,
        "labels": labels,
        "modalities": modalities,
        "video_ids": video_ids,
        "tsne_model": embedding,
        "scaler": scaler,
        "config": {
            "perplexity": perplexity,
            "metric": metric,
            "random_state": random_state,
            "standardize": standardize,
        },
    }


def build_tsne_baseline_from_npz(
    npz_path: str | Path,
    *,
    perplexity: float = 30.0,
    metric: str = "cosine",
    random_state: int = 42,
    standardize: bool = True,
) -> dict:
    """Load embeddings from .npz and fit an incremental t-SNE baseline."""
    loaded = load_embeddings_npz(npz_path)
    return build_tsne_baseline(
        loaded["video_embeddings"],
        loaded["video_labels"],
        text_embeddings=loaded["text_embeddings"],
        text_labels=loaded["text_labels"],
        perplexity=perplexity,
        metric=metric,
        random_state=random_state,
        standardize=standardize,
    )


def save_tsne_session(session: dict, output_path: str | Path) -> Path:
    """Persist a t-SNE baseline session to disk with pickle."""
    output = Path(output_path).expanduser().resolve()
    output.parent.mkdir(parents=True, exist_ok=True)
    with open(output, "wb") as f:
        pickle.dump(session, f)
    return output


def load_tsne_session(session_path: str | Path) -> dict:
    """Load a previously saved t-SNE baseline session."""
    path = Path(session_path).expanduser().resolve()
    with open(path, "rb") as f:
        session = pickle.load(f)
    return session


def plot_tsne_session(
    session: dict,
    *,
    figsize: tuple[float, float] = (11, 8),
    title: str = "Baseline t-SNE: Video/Text Embeddings",
    alpha: float = 0.85,
    s: float = 35,
    show: bool = True,
):
    """Plot baseline points from a fitted t-SNE session."""
    import matplotlib.pyplot as plt

    coords = np.asarray(session["baseline_coords"])
    labels = list(session["labels"])
    modalities = list(session["modalities"])
    video_ids = list(session["video_ids"])

    fig, ax = plt.subplots(figsize=figsize)
    unique_video_ids = sorted(set(video_ids))
    for vid in unique_video_ids:
        idx = [i for i, x in enumerate(video_ids) if x == vid]
        if not idx:
            continue
        color = _stable_color_for_video_id(vid)
        idx_video = [i for i in idx if modalities[i] == "video"]
        idx_text = [i for i in idx if modalities[i] == "text"]
        if idx_video:
            xy = coords[np.asarray(idx_video)]
            ax.scatter(xy[:, 0], xy[:, 1], c=[color], s=s, alpha=alpha, marker="o", label=vid)
        if idx_text:
            xy = coords[np.asarray(idx_text)]
            ax.scatter(xy[:, 0], xy[:, 1], c=[color], s=s + 5, alpha=alpha, marker="x")

    ax.set_title(title)
    ax.set_xlabel("t-SNE 1")
    ax.set_ylabel("t-SNE 2")

    # Keep legend compact for many videos.
    if len(unique_video_ids) <= 25:
        ax.legend(bbox_to_anchor=(1.02, 1), loc="upper left", fontsize=8)
    plt.tight_layout()
    if show:
        plt.show()
    return None if show else (fig, ax)


def overlay_tsne_points(
    session: dict,
    new_embeddings: jax.Array | np.ndarray,
    new_labels: list[str],
    *,
    modality: str = "video",
    ax=None,
    marker: str | None = None,
    s: float = 70,
    alpha: float = 0.95,
) -> np.ndarray:
    """Transform and overlay new points onto an existing baseline t-SNE axes.

    Notes:
      - Requires a baseline session fitted by build_tsne_baseline.
      - `new_labels` can be segment labels (e.g. '<video>#seg004'); points from
        the same base video id share a deterministic color.
    """
    import matplotlib.pyplot as plt

    new_np = _to_numpy_2d(new_embeddings, name="new_embeddings").astype(np.float32, copy=False)
    if len(new_labels) != new_np.shape[0]:
        raise ValueError("len(new_labels) must match new_embeddings rows")

    scaler = session.get("scaler")
    tsne_model = session.get("tsne_model")
    if tsne_model is None:
        raise ValueError("Session is missing `tsne_model`; cannot transform new points.")

    new_proc = scaler.transform(new_np) if scaler is not None else new_np
    new_xy = np.asarray(tsne_model.transform(new_proc), dtype=np.float32)
    new_video_ids = [_strip_segment_suffix(lab) for lab in new_labels]

    if ax is None:
        _fig, ax = plt.subplots(figsize=(8, 6))

    point_marker = marker if marker is not None else ("^" if modality == "video" else "P")
    for vid in sorted(set(new_video_ids)):
        idx = [i for i, x in enumerate(new_video_ids) if x == vid]
        xy = new_xy[np.asarray(idx)]
        color = _stable_color_for_video_id(vid)
        ax.scatter(
            xy[:, 0],
            xy[:, 1],
            c=[color],
            s=s,
            alpha=alpha,
            marker=point_marker,
            edgecolors="k",
            linewidths=0.7,
        )

    return new_xy


# ---------------------------------------------------------------------------
# Checkpoint download helper
# ---------------------------------------------------------------------------

def download_checkpoint(
    gdrive_url: str,
    output_path: str | Path = "params.msgpack",
) -> Path:
    """Download a model checkpoint from Google Drive.

    Uses ``gdown`` (included in project dependencies).

    Args:
        gdrive_url: A Google Drive sharing URL.
        output_path: Local path to save the file.

    Returns:
        Path to the downloaded checkpoint.
    """
    import gdown

    out = Path(output_path)
    if out.exists():
        print(f"Checkpoint already exists at {out}, skipping download.")
        return out

    print(f"Downloading checkpoint to {out} ...")
    gdown.download(gdrive_url, str(out), fuzzy=True)
    print("Download complete.")
    return out
